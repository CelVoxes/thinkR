return(list(
title = if (is_final_answer) "Final Answer" else "Reasoning Step",
content = parsed_response$response,
next_action = if (is_final_answer) "final_answer" else "continue"
))
},
error_response = function(error_msg, is_final_answer) {
# Generate an error response
return(list(
title = "Error",
content = sprintf(
"Failed to generate %s after %d attempts. Error: %s",
ifelse(is_final_answer, "final answer", "step"),
self$max_attempts,
error_msg
),
next_action = ifelse(is_final_answer, "final_answer", "continue")
))
}
)
)
# OllamaHandler class (subclass of BaseHandler)
OllamaHandler <- R6::R6Class(
"OllamaHandler",
inherit = BaseHandler,
public = list(
model = "llama3.1",
temperature = 0.7,
top_p = 0.9,
initialize = function(model = "llama3.1", temperature = 0.7, top_p = 0.9) {
self$model <- model
self$temperature <- temperature
self$top_p <- top_p
},
make_request = function(messages, max_tokens) {
prompt <- paste(sapply(messages, function(m) paste(m$role, m$content, sep = ": ")), collapse = "\n")
data <- list(
model = self$model,
prompt = prompt,
stream = FALSE
)
response <- httr::POST("http://localhost:11434/api/generate",
body = data,
encode = "json"
)
if (httr::status_code(response) != 200) {
stop("Error in Ollama API call: ", httr::content(response, "text"))
}
content <- httr::content(response, "text")
return(content)
},
process_response = function(response, is_final_answer) {
# Parse the outer JSON structure
parsed_response <- jsonlite::fromJSON(response)
# Function to extract and parse JSON objects from text
extract_json <- function(text) {
json_pattern <- "\\{[^{}]*\\}"
matches <- gregexpr(json_pattern, text, perl = TRUE)
json_strings <- regmatches(text, matches)[[1]]
parsed_objects <- lapply(json_strings, function(json_str) {
tryCatch(
jsonlite::fromJSON(json_str),
error = function(e) NULL
)
})
# Remove NULL entries (failed parses)
parsed_objects <- Filter(Negate(is.null), parsed_objects)
return(parsed_objects)
}
# Extract and parse all JSON objects from the response
parsed_objects <- extract_json(parsed_response$response)
# Combine all parsed objects
combined_response <- list(
title = if (length(parsed_objects) > 0 && !is.null(parsed_objects[[1]]$title)) {
parsed_objects[[1]]$title
} else if (is_final_answer) "Final Answer" else "Reasoning Step",
content = paste(sapply(parsed_objects, function(obj) obj$content), collapse = "\n"),
next_action = if (length(parsed_objects) > 0 && !is.null(parsed_objects[[length(parsed_objects)]]$next_action)) {
parsed_objects[[length(parsed_objects)]]$next_action
} else if (is_final_answer) "final_answer" else "continue"
)
return(combined_response)
}
)
)
# Function to generate response (similar to the Python version)
generate_response <- function(prompt, api_handler) {
# Initialize conversation
messages <- list(
list(role = "system", content = paste(thinkR::SYSTEM_PROMPT, collapse = "\n")),
list(role = "user", content = prompt),
list(role = "assistant", content = "Understood. I will now create a detailed reasoning chain following the given instructions, starting with a thorough problem decomposition.")
)
steps <- list()
step_count <- 1
total_thinking_time <- 0
lapply(messages, function(m) message(crayon::bold(enc2utf8(m$role)), ": ", crayon::silver(enc2utf8(m$content))))
# Main loop for generating reasoning steps
repeat {
start_time <- Sys.time()
step_data <- api_handler$make_api_call(messages, 300)
end_time <- Sys.time()
thinking_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
total_thinking_time <- total_thinking_time + thinking_time
# Store step information
steps[[length(steps) + 1]] <- list(
title = paste("Step", step_count, ":", step_data$title),
content = step_data$content,
thinking_time = thinking_time
)
# Add assistant's response to conversation
messages[[length(messages) + 1]] <- list(role = "assistant", content = step_data$content)
# Safely print the assistant's response
message(
crayon::bold("assistant: "),
crayon::italic(crayon::silver(toString(step_data$title))), "\n",
crayon::silver(toString(step_data$content)), "\n"
)
# Check for next_action
next_action <- tolower(trimws(step_data$next_action))
message("Next reasoning step: ", next_action)
if (is.null(step_data$content) || trimws(toString(step_data$content)) == "") {
message("Warning: Received empty response.")
step_count <- step_count + 1
next
}
if (step_count > 25) {
message("Maximum step count reached. Exiting loop.")
break
}
# Break loop if it's the final answer or step count exceeds 10
if (next_action == "final_answer") {
break
}
step_count <- step_count + 1
}
# If we've reached this point, we already have the final answer
final_data <- step_data
# Add final answer to steps (if it's not already there)
if (steps[[length(steps)]]$title != "Final Answer") {
steps[[length(steps) + 1]] <- list(
title = "Final Answer",
content = final_data$content,
thinking_time = thinking_time
)
}
message("\nFinal answer: ", crayon::silver(final_data$content))
# Return final results
return(list(steps = steps, total_thinking_time = total_thinking_time))
}
## Usage example
# handler <- OllamaHandler$new()
# result <- generate_response("What is the capital of France?", handler)
# print(result)
ollama <- OllamaHandler$new(model = "llama3.1")
result <- generate_response("How many 'R's are in strawberry?", ollama)
library(jsonlite)
library(httr)
# BaseHandler class (using R6 for object-oriented programming)
BaseHandler <- R6::R6Class(
"BaseHandler",
public = list(
max_attempts = 3, # Maximum number of retry attempts
retry_delay = 1, # Delay between retry attempts in seconds
initialize = function() {
# Constructor
},
make_api_call = function(messages, max_tokens, is_final_answer = FALSE) {
# Attempt to make an API call with retry logic
for (attempt in 1:self$max_attempts) {
tryCatch(
{
response <- self$make_request(messages, max_tokens)
return(self$process_response(response, is_final_answer))
},
error = function(e) {
if (attempt == self$max_attempts) {
return(self$error_response(toString(e), is_final_answer))
}
Sys.sleep(self$retry_delay)
}
)
}
},
make_request = function(messages, max_tokens) {
# This method should be implemented in a subclass for Ollama
stop("make_request must be implemented in a subclass")
},
process_response = function(response, is_final_answer) {
# Default response processing (can be overridden by subclasses)
parsed_response <- fromJSON(response)
return(list(
title = if (is_final_answer) "Final Answer" else "Reasoning Step",
content = parsed_response$response,
next_action = if (is_final_answer) "final_answer" else "continue"
))
},
error_response = function(error_msg, is_final_answer) {
# Generate an error response
return(list(
title = "Error",
content = sprintf(
"Failed to generate %s after %d attempts. Error: %s",
ifelse(is_final_answer, "final answer", "step"),
self$max_attempts,
error_msg
),
next_action = ifelse(is_final_answer, "final_answer", "continue")
))
}
)
)
# OllamaHandler class (subclass of BaseHandler)
OllamaHandler <- R6::R6Class(
"OllamaHandler",
inherit = BaseHandler,
public = list(
model = "llama3.1",
temperature = 0.7,
top_p = 0.9,
initialize = function(model = "llama3.1", temperature = 0.7, top_p = 0.9) {
self$model <- model
self$temperature <- temperature
self$top_p <- top_p
},
make_request = function(messages, max_tokens) {
prompt <- paste(sapply(messages, function(m) paste(m$role, m$content, sep = ": ")), collapse = "\n")
data <- list(
model = self$model,
prompt = prompt,
stream = FALSE
)
response <- httr::POST("http://localhost:11434/api/generate",
body = data,
encode = "json"
)
if (httr::status_code(response) != 200) {
stop("Error in Ollama API call: ", httr::content(response, "text"))
}
content <- httr::content(response, "text")
return(content)
},
process_response = function(response, is_final_answer) {
# Parse the outer JSON structure
parsed_response <- jsonlite::fromJSON(response)
# Function to extract and parse JSON objects from text
extract_json <- function(text) {
json_pattern <- "\\{[^{}]*\\}"
matches <- gregexpr(json_pattern, text, perl = TRUE)
json_strings <- regmatches(text, matches)[[1]]
parsed_objects <- lapply(json_strings, function(json_str) {
tryCatch(
jsonlite::fromJSON(json_str),
error = function(e) NULL
)
})
# Remove NULL entries (failed parses)
parsed_objects <- Filter(Negate(is.null), parsed_objects)
return(parsed_objects)
}
# Extract and parse all JSON objects from the response
parsed_objects <- extract_json(parsed_response$response)
# Combine all parsed objects
combined_response <- list(
title = if (length(parsed_objects) > 0 && !is.null(parsed_objects[[1]]$title)) {
parsed_objects[[1]]$title
} else if (is_final_answer) "Final Answer" else "Reasoning Step",
content = paste(sapply(parsed_objects, function(obj) obj$content), collapse = "\n"),
next_action = if (length(parsed_objects) > 0 && !is.null(parsed_objects[[length(parsed_objects)]]$next_action)) {
parsed_objects[[length(parsed_objects)]]$next_action
} else if (is_final_answer) "final_answer" else "continue"
)
return(combined_response)
}
)
)
# Function to generate response (similar to the Python version)
generate_response <- function(prompt, api_handler) {
# Initialize conversation
messages <- list(
list(role = "system", content = paste(thinkR::SYSTEM_PROMPT, collapse = "\n")),
list(role = "user", content = prompt),
list(role = "assistant", content = "Understood. I will now create a detailed reasoning chain following the given instructions, starting with a thorough problem decomposition.")
)
steps <- list()
step_count <- 1
total_thinking_time <- 0
lapply(messages, function(m) message(crayon::bold(enc2utf8(m$role)), ": ", crayon::silver(enc2utf8(m$content))))
# Main loop for generating reasoning steps
repeat {
start_time <- Sys.time()
step_data <- api_handler$make_api_call(messages, 300)
end_time <- Sys.time()
thinking_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
total_thinking_time <- total_thinking_time + thinking_time
# Store step information
steps[[length(steps) + 1]] <- list(
title = paste("Step", step_count, ":", step_data$title),
content = step_data$content,
thinking_time = thinking_time
)
# Add assistant's response to conversation
messages[[length(messages) + 1]] <- list(role = "assistant", content = step_data$content)
# Safely print the assistant's response
message(
crayon::bold("assistant: "),
crayon::italic(crayon::silver(toString(step_data$title))), "\n",
crayon::silver(toString(step_data$content)), "\n"
)
# Check for next_action
next_action <- tolower(trimws(step_data$next_action))
message("Next reasoning step: ", next_action, "\n")
if (is.null(step_data$content) || trimws(toString(step_data$content)) == "") {
message("Warning: Received empty response.")
step_count <- step_count + 1
next
}
if (step_count > 25) {
message("Maximum step count reached. Exiting loop.")
break
}
# Break loop if it's the final answer or step count exceeds 10
if (next_action == "final_answer") {
break
}
step_count <- step_count + 1
}
# If we've reached this point, we already have the final answer
final_data <- step_data
# Add final answer to steps (if it's not already there)
if (steps[[length(steps)]]$title != "Final Answer") {
steps[[length(steps) + 1]] <- list(
title = "Final Answer",
content = final_data$content,
thinking_time = thinking_time
)
}
message(crayon::green("Final answer: "), crayon::silver(final_data$content))
# Return final results
return(list(steps = steps, total_thinking_time = total_thinking_time))
}
## Usage example
# handler <- OllamaHandler$new()
# result <- generate_response("What is the capital of France?", handler)
# print(result)
ollama <- OllamaHandler$new(model = "llama3.1")
result <- generate_response("How many 'R's are in strawberry?", ollama)
devtools::install_github("eonurk/thinkR", force = T, upgrade = T)
library(thinkR)
ollama <- OllamaHandler$new(model = "llama3.1")
devtools::install_github("eonurk/thinkR", force = T, upgrade = T)
result <- generate_response("How many 'R's are in strawberry?", ollama)
result$total_thinking_time
result$total_thinking_time
rstudioapi::insertText()
library(thinkR)
devtools::document()
warnings()
devtools::document()
devtools::install()
thinkr_addin
thinkr_addin("hello!")
library(thinkR)
thinkr_addin("hello!")
thinkr_addin()
reprex:::reprex_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
ui <- shiny::fluidPage(
shiny::tags$head(
shiny::tags$link(rel = "stylesheet", type = "text/css", href = "thinkr_styles.css"),
shiny::tags$script(src = "https://kit.fontawesome.com/your-fontawesome-kit.js") # Replace with your Font Awesome kit
),
shiny::div(
class = "container-fluid p-0",
shiny::div(
class = "row no-gutters",
shiny::div(
class = "col-md-3 sidebar",
shiny::h2("ThinkR", class = "mb-4"),
shiny::selectInput("model", "Model:",
choices = c("llama2", "codellama", "mistral", "llama3.1"),
selected = "llama3.1"
),
shiny::sliderInput("temperature", "Temperature:",
value = 0.7, min = 0, max = 1, step = 0.1
),
shiny::sliderInput("top_p", "Top P:",
value = 0.9, min = 0, max = 1, step = 0.1
),
shiny::hr(),
shiny::actionButton("clear_chat", "Clear Chat", class = "btn btn-block btn-outline-danger")
),
shiny::div(
class = "col-md-9 main-content",
shiny::div(id = "chat_container", class = "chat-container"),
shiny::div(
class = "input-area",
shiny::textAreaInput("user_input", NULL, rows = 3, placeholder = "Enter your question or code..."),
shiny::actionButton("submit", NULL, icon = shiny::icon("paper-plane"), class = "btn btn-primary btn-send")
)
)
)
)
)
source("~/Desktop/thinkR/R/thinkR_addin.R")
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(shinyjs)
install.packages("shinyjs")
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
gc()
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
clear
clear()
clear()
clc()
clc
ls
library(thinkR)
thinkR::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
library(thinkR)
thinkR:::thinkr_addin()
